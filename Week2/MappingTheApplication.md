# Mapping the Application

One of the first processes with hacking any web application is mapping it. The idea behind this is the ability to find its strengths and weaknesses. This allows for possible enumeration where possible thanks to finding the inner workings within the application.

## Directory

**[Web Spidering](#Web-Spidering)**<br>
**[User-Directed Spiders](#User-Directed-Spiders)**<br>
**[Discovering Hidden Content](#Discovering-Hidden-Content)**<br>
**[]()**<br>
**[]()**<br>
**[]()**<br>
**[]()**<br>
**[]()**<br>

## Web Spidering

This is the use of tools to help enumerate and map web applications. Numerous tools exist and some include: Burp Suite, WebScarab, Zed Attack Proxy (ZAP), and CAT.

These tools will parse through the HTML, find forms, and trying different inputs with checking responses to potentially find vulnerabilities. They might looks for common directories that contain potentially vulnerable or sensitive pages.

These tools may also have their downfalls. Though easy to operate, these tools will miss much of what the application may have to offer. These are a couple of the things that the application will miss:

1. Unusual navigation mechanisms. These can include dynamically generated pages.
1. Links buried within compiled client-side objects such as flash or java applets.
1. There may be a multistage functionality within the application that will not accept values put in by an automated tool.
1. The same URL may be used by an application for a multiple of forms. This may be a downfall as the automated tool may fingerprint the URL as one form, and after being used, when a different form under the same URL is give, may not try any inputs. This means that a whole side of the application may not even be tested.
1. Some appications may generate unique URLs that the tool cannot differentiate from and thus run indefinitely as it handles the same form as a different one as it has a different URL.
1. The spider needs to have the ability to authenticate itself whereever needed. If a form needs a token of sorts to execute correctly, then it must have it preconfigured. There are a number of weird conditions and circumstances that can be done:
    1. The Spider may log itself out as it finds the POST/GET request in doing so terminating the session.
    1. If the spider submits invalid input into a sensitive function, the application may terminate the session for defensive reasons.
    1. The Application may use per-page tokens. The spider in nearly all circumstances will fail in this aspect.

## User-Directed Spiders

This is a little more sophisticated methods where the user runs through the website as normal and the traffic is sent through the spider/proxy. 

The tool then builds a map of the application and incorporates all URLs found from the user browsing the pages. Two of the two mentions as spiders earlier: Burp Suite and WebScarab have this ability. There are numerous benefits over normal spiders, these include when:

1. The application uses unique, complex, or unusual mechanisms for navigation.
1. The user controls all the data sent into the application making sure all input validation is met.
1. The user logs into the application normally, this ensures that the authentication process remains active throughout the mapping process.
1. Any dangerous functionality such as "deleteUser.jsp" is also mapped, but th user will choose not to run the function and thus it will not be executed.

## Discovering Hidden Content

This is content that is not directly linked or reachable from the original content. This usually is content or code that was written for testing or debugging purposes. This is also common for applications with many levels of horizontal priveleges. This being the ones with anonymous, normal, and admin types roles. 

Thus the spidering approach may not find the content that is available at admin level while browsing at a normal level. To discover such content can be possible, but will need a certain degree of luck.

### Brute Forcing Techniques

This is the act of making requests to directories within the site that may or may not exist. Thus, if a page is contacted, the response could be a 200. This would suggest that page exists. But more likely it will come back with a 404 response. There is another chance that it may be 403 or 302, which means that the page exists and the user trying to access it isn't allowed.

The book recommends to use Burp Suite's Intruder. But there are better ways. A common one is to use DirBuster, found on GitHub, then pipe the output into httprobe, also found on GitHub.

### Inference from Public Content

Try to find patterns in the naming convention of resources. If an application is using a certain naming scheme, then try to make a wordlist that follows it.
