# Mapping the Application

One of the first processes with hacking any web application is mapping it. The idea behind this is the ability to find its strengths and weaknesses. This allows for possible enumeration where possible thanks to finding the inner workings within the application.

## Directory

**[Web Spidering](#Web-Spidering)**<br>
**[User-Directed Spiders](#User-Directed-Spiders)**<br>
**[Discovering Hidden Content](#Discovering-Hidden-Content)**<br>
**[Application Pages Versus Functional Paths](#Application-Pages-Versus-Functional-Paths)**<br>
**[]()**<br>
**[]()**<br>
**[]()**<br>
**[]()**<br>

## Web Spidering

This is the use of tools to help enumerate and map web applications. Numerous tools exist and some include: Burp Suite, WebScarab, Zed Attack Proxy (ZAP), and CAT.

These tools will parse through the HTML, find forms, and trying different inputs with checking responses to potentially find vulnerabilities. They might looks for common directories that contain potentially vulnerable or sensitive pages.

These tools may also have their downfalls. Though easy to operate, these tools will miss much of what the application may have to offer. These are a couple of the things that the application will miss:

1. Unusual navigation mechanisms. These can include dynamically generated pages.
1. Links buried within compiled client-side objects such as flash or java applets.
1. There may be a multistage functionality within the application that will not accept values put in by an automated tool.
1. The same URL may be used by an application for a multiple of forms. This may be a downfall as the automated tool may fingerprint the URL as one form, and after being used, when a different form under the same URL is give, may not try any inputs. This means that a whole side of the application may not even be tested.
1. Some appications may generate unique URLs that the tool cannot differentiate from and thus run indefinitely as it handles the same form as a different one as it has a different URL.
1. The spider needs to have the ability to authenticate itself whereever needed. If a form needs a token of sorts to execute correctly, then it must have it preconfigured. There are a number of weird conditions and circumstances that can be done:
    1. The Spider may log itself out as it finds the POST/GET request in doing so terminating the session.
    1. If the spider submits invalid input into a sensitive function, the application may terminate the session for defensive reasons.
    1. The Application may use per-page tokens. The spider in nearly all circumstances will fail in this aspect.

## User-Directed Spiders

This is a little more sophisticated methods where the user runs through the website as normal and the traffic is sent through the spider/proxy. 

The tool then builds a map of the application and incorporates all URLs found from the user browsing the pages. Two of the two mentions as spiders earlier: Burp Suite and WebScarab have this ability. There are numerous benefits over normal spiders, these include when:

1. The application uses unique, complex, or unusual mechanisms for navigation.
1. The user controls all the data sent into the application making sure all input validation is met.
1. The user logs into the application normally, this ensures that the authentication process remains active throughout the mapping process.
1. Any dangerous functionality such as "deleteUser.jsp" is also mapped, but th user will choose not to run the function and thus it will not be executed.

## Discovering Hidden Content

This is content that is not directly linked or reachable from the original content. This usually is content or code that was written for testing or debugging purposes. This is also common for applications with many levels of horizontal priveleges. This being the ones with anonymous, normal, and admin types roles.  

Thus the spidering approach may not find the content that is available at admin level while browsing at a normal level. To discover such content can be possible, but will need a certain degree of luck. 

A tool that may help with this process is to use a tool such as Nikto. It tries to find some common hidden paths and files.

### Brute Forcing Techniques

This is the act of making requests to directories within the site that may or may not exist. Thus, if a page is contacted, the response could be a 200. This would suggest that page exists. But more likely it will come back with a 404 response. There is another chance that it may be 403 or 302, which means that the page exists and the user trying to access it isn't allowed.

The book recommends to use Burp Suite's Intruder. But there are better ways. A common one is to use OWASP's DirBuster, found on GitHub, then pipe the output into httprobe, also found on GitHub.

### Inference from Public Content

Try to find patterns in the naming convention of resources. If an application is using a certain naming scheme, then try to make a wordlist that follows it.

### Use of Public Information

There are resources out there that are rather simple to use to gather more information than certain tools can get. These include:

1. Search Engines:
    - Google, MSN, and Yahoo have many spiders that are consistantly crawling through the internet and indexing information that may have not wanted to be indexed.
    - A good example of this being used is Google Dorking
        - site:www.example.com returns every resource within the target site that Google has a reference to.
        - site:www.example.com login returns all the pages containing the expression login. In a large and complex application, this technique can be used to quickly home in on interesting resources, such as site maps, password reset functions, and administrative menus.
        - link:www.example.com returns all the pages on other websites and applications that contain a link to the target. This may include links to old content, or functionality that is intended for use only by third par- ties, such as partner links.
        - related:www.example.com returns pages that are “similar” to the target and therefore includes a lot of irrelevant material. However, it may also discuss the target on other sites, which may be of interest.
1. Web Archives:
    - Such as the WayBack Machine can be used to who sites from a preselected time in the past.
    - Also one thing to note is that the site may talk of a relation to how it interacts with a third party. This in turn can lead you to the third party's site which could explain technological aspects of the first company that wasn't on the first party's own site.

### Leveraging the Web Server

The web server itself may be vulnerable to specific types of attacks that could list out the contents of a directory or the like. This is rather traditional pentesting in nature.

## Discovering Hidden Parameters

There may be hidden parameters that can alter how the application runs. An example for this may be when adding: ```debug=true```. The application may allow for some data, or even show more.

## Analyzing the Application

This is the act of enumerating as much as possible. Some key areas within this are:

- The Application's core functionality (the actions that can be leveraged to perform when used as intended)
- Other, more peripheral application behavior, including off-site links, error messages, admin and logging functions, and the use of redirects.
- Core security mechanisms and how they function. Thus, how is session state managed, access controls, auth mechanisms, and supporting logic (user reg, password updating, and account recovery.)
- The different locations at which the application processes user-supplied input
- The tech employed on the client side (forms, scripts, thick-clients, and cookies)
- Any details that may be gleaned about the internal structure and functionality of the server side component.

## Identifying Entry Points for User Input

Common entry points include:
- Every URL string up to the query string marker
- Every parameter submitted within the URL query string
- Every parameter submitted within the body of a POST request
- Every cookie
- Every other HTTP header that the application might process (user-agent, referer, accept, accept-language, and host)



